{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API REQUEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup   \n",
    "\n",
    "\"\"\" Set up the parameters we want to pass to the API. This is the latitude and longitude of New York City. \"\"\"\n",
    "parameters = {\"lat\": 40.71, \"lon\": -74}\n",
    "\n",
    "\n",
    "# Make a get request with the pahttp://api.open-notify.orgrameters. \n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "\n",
    "\"\"\"  is the response positive? \"\"\" \n",
    "print(response)\n",
    "\n",
    "# Print the content of the response (the data the server returned)\n",
    "#print(response.content)\n",
    "\n",
    "# save response into a much nicer format\n",
    "a = response.json()\n",
    "#print(response.json())\n",
    "\n",
    "pretty_json = json.dumps(a, indent=4, sort_keys=True)\n",
    "print(pretty_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the same request we did earlier, but with the coordinates of San Francisco instead.\n",
    "\n",
    "parameters = {\"lat\": 37.78, \"lon\": -122.41}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)\n",
    "# Get the response data as a python object. Verify that it's a dictionary.\n",
    "data = response.json()\n",
    "#print(type(data))\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "pretty_json = json.dumps(data, indent=4, sort_keys=True)\n",
    "print(pretty_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"latlong_ny.json\", \"w\") as fp:\n",
    "    json.dump(a,fp) \n",
    "\n",
    "with open('latlong.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for p in data['response']:\n",
    "        print(f\"Duration: {p['duration']}\")\n",
    "        print(f\"RiseTime: {p['risetime']}\")\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are Name, Appearance, Origin\n",
      "\t The beer Edelweiss is White, and it is from Austria.\n",
      "\t The beer Cuvée des Trolls is Blond, and it is from Belgium.\n",
      "\t The beer Choulette Ambrée is Amber, and it is from France.\n",
      "\t The beer Gulden Draak is Dark, and it is from Belgium.\n",
      "\t The beer Water is Crystal Clear, and it is from Anywhere.\n",
      "Processed 6 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('./data/beers.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            print(f'\\t The beer {row[0]} is {row[1]}, and it is from {row[2]}.')\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV into Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are Name, Appearance, Origin\n",
      "\tThe beer Edelweiss is White, and it is from Austria.\n",
      "\tThe beer Cuvée des Trolls is Blond, and it is from Belgium.\n",
      "\tThe beer Choulette Ambrée is Amber, and it is from France.\n",
      "\tThe beer Gulden Draak is Dark, and it is from Belgium.\n",
      "\tThe beer Water is Crystal Clear, and it is from Anywhere.\n",
      "Processed 6 lines.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('./data/beers.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        print(f'\\tThe beer {row[\"Name\"]} is {row[\"Appearance\"]}, and it is from {row[\"Origin\"]}.')\n",
    "        line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Power of Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Name     Appearance    Origin\n",
      "0         Edelweiss          White   Austria\n",
      "1  Cuvée des Trolls          Blond   Belgium\n",
      "2  Choulette Ambrée          Amber    France\n",
      "3      Gulden Draak           Dark   Belgium\n",
      "4             Water  Crystal Clear  Anywhere\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv('./data/beers.csv')\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downloading html with requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can print out the HTML content of the page using the content property: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "page\n",
    "\n",
    "page.status_code\n",
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Weather.gov\n",
    "    \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.X9DVpBakolQ\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "html = list(soup.children)[2]\n",
    "body = list(html.children)[3]\n",
    "\n",
    "print(body)\n",
    "\n",
    "#target parents\n",
    "#go deeper into childen\n",
    "#find p element or relevant class\n",
    "\n",
    "    #target day\n",
    "    #target temperature\n",
    "    #target summary \n",
    "    \n",
    "#save into a list\n",
    "#save into a dictionary?\n",
    "\n",
    "{\"day1\":[\"sunday\", \"54\", 'today was a great sunny day '], \"day2\":[\"monday\", \"54\", 'today was a great sunny day ']}\n",
    "\n",
    "#load dict into pandas\n",
    "# create column names if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This are the children\n",
      "\n",
      "Here is some simple content for this page.\n"
     ]
    }
   ],
   "source": [
    "# For further information\n",
    "# https://www.dataquest.io/blog/python-api-tutorial/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "# Print the status code of the response.\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "\n",
    "print()\n",
    "print(\"This are the children\")\n",
    "print()\n",
    "\n",
    "html = list(soup.children)[2]\n",
    "#print(html)\n",
    "\n",
    "body = list(html.children)[3]\n",
    "#print(body)\n",
    "\n",
    "p = list(body.children)[1]\n",
    "#print(p)\n",
    "\n",
    "#print(p.get_text())\n",
    "\n",
    "#Faster Way\n",
    "\n",
    "p = soup.find('p')\n",
    "\n",
    "print(p.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['html', '\\n', \"<html> <head> <title>A simple example page</title></head><body><p>Here is some simple content for this page.</p></body></html>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html> <head> <title>A simple example page</title></head><body><p>Here is some simple content for this page.</p></body></html>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FindAll with BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                First outer paragraph.\n",
      "            </b>\n",
      "</p>\n"
     ]
    }
   ],
   "source": [
    "# https://www.dataquest.io/blog/python-api-tutorial/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page =requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "# Print the status code of the response.\n",
    "\n",
    "soup =BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "\n",
    "outer = soup.find_all('p', class_='outer-text')[0]\n",
    "#outer = soup.find(id='first').string\n",
    "print(outer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer.find_all('b')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a29e2c6032db948d2ac4841a3008c24c124c9d60225cd5bb2e940e04b511554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
